# Tips and Tricks for Training FastAI Models

1. TOC
{:toc}

## Gradient accumulation
By passing the `cbs` parameter to `vision_learner` it is possible to use gradient accumulation to reduce GPU memory usage. Gradient accumulation while training means that instead of updating the weights after every iteration, we will instead accumulate the gradients for `n` batches. Theoretically, the result will be identical as the underlying mathematical model remains unchanged; however, memory consumption is expected to be reduced by a factor of `n`.

In FastAI this is done using `GradientAccumulation`.


## Loss function
Picking an appropriate loss function can improve your fine tuning results so they are absolutely worth investigating. They are used to estimate the amount of error for the model and as such are evaluated repeatedly during training. A lot of this paragraph comes from a similar blog post[^1]. They can be broken into three broad categories, these are:

1. Regression Loss Functions
    - Mean Squared Error Loss
    - Mean Squared Logarithmic Error Loss
    - Mean Absolute Error Loss
2. Binary Classification Loss Functions
    - Binary Cross-Entropy
    - Hinge Loss
    - Squared Hinge Loss
3. Multi-Class Classification Loss Functions
    - Multi-Class Cross-Entropy Loss
    - Sparse Multiclass Cross-Entropy Loss
    - Kullback Leibler Divergence Loss

These categories will be discussed further below.

## Regression
These types of loss functions are used to predict a continuous quantity, this will often come as floating point data but may not (for example; height rounded to the nearest cm).

<div style="text-align:center">
    <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ae01f23f2ed2814c7db94e1fdc952732f5ca533a" />
</div>

## Binary classification
These are used to estimate the error for a variable that has a discrete, binary value (e.g. left-handedness). These core functions usually take their input as the binary `0` or `1` but will often be extended to a user-level function that takes two categories as strings (at least, with FastAI and PyTorch).

## Multi-class classification
Often these will be used for multi-class error estimation (e.g. species of animal). Discrete, ordered, numerical measurements, even if they are not actually continuous, are better estimated by regression loss functions (e.g. number of steps in a flight of stairs).

[^1]:[How to Choose Loss Functions When Training Deep Learning Neural Networks](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)
