# Tips and Tricks for Training FastAI Models (Part 1)

1. TOC
{:toc}

## Gradient accumulation
By passing the `cbs` parameter to `vision_learner` it is possible to use gradient accumulation to reduce GPU memory usage. Gradient accumulation while training means that instead of updating the weights after every iteration, we will instead accumulate the gradients for `n` batches. Theoretically, the result will be identical as the underlying mathematical model remains unchanged; however, memory consumption is expected to be reduced by a factor of `n`.

In FastAI this is done using `GradientAccumulation`.


## Loss function
Picking an appropriate loss function can improve your fine tuning results so they are absolutely worth investigating. They are used to estimate the amount of error for the model and as such are evaluated repeatedly during training. A lot of this paragraph comes from a similar blog post[^1]. They can be broken into three broad categories, these are:

1. Regression Loss Functions
    - Mean Squared Error Loss
    - Mean Squared Logarithmic Error Loss
    - Mean Absolute Error Loss
2. Binary Classification Loss Functions
    - Binary Cross-Entropy
    - Hinge Loss
    - Squared Hinge Loss
3. Multi-Class Classification Loss Functions
    - Multi-Class Cross-Entropy Loss
    - Sparse Multiclass Cross-Entropy Loss
    - Kullback Leibler Divergence Loss

These categories will be discussed further below.

## Regression
These types of loss functions are used to predict a continuous quantity, this will often come as floating point data but may not (for example; height rounded to the nearest cm). The equation for calculating regression loss (mean squared error) is below[^2] where $Y$ is the observed values and $\hat Y$ is the predicted values.

$$MSE = \frac 1 n \sum_{i=1}^n (Y - \hat Y)^2$$

## Binary classification
These are used to estimate the error for a variable that has a discrete, binary value (e.g. left-handedness). These core functions usually take their input as the binary `0` or `1` but will often be extended to a user-level function that takes two categories as strings (at least, with FastAI and PyTorch). The equation for calculating binary classification loss or cross entropy is given below[^3] where $p$ is the observed values and $q$ is the predicted values.

![cross-entroy equation](/images/cross-entropy-equation.svg)

## Multi-class classification
Often these will be used for multi-class error estimation (e.g. species of animal). Discrete, ordered, numerical measurements, even if they are not actually continuous, are better estimated by regression loss functions (e.g. number of steps in a flight of stairs). The equation for calculating this is the same as binary classification (cross-entropy) above except it is calculated for all classes and the results are averaged to give the total error.

[^1]: [How to Choose Loss Functions When Training Deep Learning Neural Networks](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)
[^2]: Courtesy of Wikipedia: [Mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error)
[^3]: Courtesy of Wikipedia: [Cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy)
