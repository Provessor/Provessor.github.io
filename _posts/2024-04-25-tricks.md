# Tips and Tricks for Training FastAI Models

1. TOC
{:toc}

## Gradient accumulation
By passing the `cbs` parameter to `vision_learner` it is possible to use gradient accumulation to reduce GPU memory usage. Gradient accumulation while training means that instead of updating the weights after every iteration, we will instead accumulate the gradients for `n` batches. Theoretically, the result will be identical as the underlying mathematical model remains unchanged; however, memory consumption is expected to be reduced by a factor of `n`.

In FastAI this is done using `GradientAccumulation`.


## Loss function
